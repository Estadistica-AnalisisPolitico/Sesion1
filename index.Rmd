---
title: "Sesión 1"
output:
  html_document:
    df_print: paged
---

<center><img src="https://github.com/Estadistica-AnalisisPolitico/Images/raw/main/LogoEAP.png" width="500"></center>



Profesor:  <a href="http://www.pucp.edu.pe/profesor/jose-manuel-magallanes/" target="_blank">Dr. José Manuel MAGALLANES REYES, Ph.D.</a> <br>

- Profesor del Departamento de Ciencias Sociales, Sección de Ciencia Política y Gobierno.

- [Oficina 105](https://goo.gl/maps/xuGeG6o9di1i1y5m6) - Edificio CISEPA / ECONOMIA / CCSS

- Telefono: (51) 1 - 6262000 anexo 4302

- Correo Electrónico: [jmagallanes@pucp.edu.pe](mailto:jmagallanes@pucp.edu.pe)
    

<a id='beginning'></a>


____

<center> <header><h2>La Ruta hacia la Regresión</h2>  </header></center>

<center><a href="https://doi.org/10.5281/zenodo.7015029"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.7015029.svg" alt="DOI"></a>
</center>

____

## El _Pipeline_ del Analista Político

De manera abstracta podemos ver que Los analistas existen para brindar __explicaciones__ y __recomendaciones__ sobre algún asunto de interés. El _decisor_ elije el curso de acción sabiendo que no puede esperar del analista información completa. El _gestor_ se encarga de implementar  las decisiones; esa implementación traerá nueva información y el analista vuelve a su trabajo.

La estadística es una ciencia matemática que guía científicamente el análisis de datos. Esto lo hace siguiendo una secuencia:

1. Apuesta por entender una variable que explicaría un problema de interés (narcotráfico, elecciones, etc). En esta etapa, hace exploración de los datos de esa variable, es decir, organiza los datos en medidas, tablas y gráficos. Se entiende que la variable de interés tiene una variabilidad tal que despierta en el analista la necesidad de preguntar por qué esa variabilidad.

2. Se plantea hipótesis respecto a qué se relaciona con la variabilidad de la variable de interés. Las hipótesis se formula no antes de haber revisado la literatura. En esta etapa hace uso de análisis bivariado o multivariado. Esta etapa se enriquece si está actualizado en las teorías que proponen cierta asociación de la variable de interés con otras variables.


3. Aplica la técnica estadística que corresponda, tal que verifique la hipótesis que se planteó. 

4. Interpretado los resultados obtenidos.

5. Elabora síntesis de lo actuado; propone explicaciones a lo encontrado; y elabora recomendaciones.

Hay muchas opciones para las _técnicas_ señaladas en el punto 4. La elección de las mismas dependerá de la preparación del analista. Esta sesión te guiará para que:

1. Recuerdes cómo y para qué hacer exploración univariada ([ir](#eda))
2. Recuerdes cómo y para qué hacer análisis bivariada ([ir](#corr)).
3. Introducir el concepto (y necesidad) de la regresión multivariada ([ir](#rlin)).


<a id='eda'></a>

## I. Explorando la Variable de interés

Supongamos que estás interesado en la "situación de los locales escolares en el Perú". Ese simple interés te lleva a buscar datos para saber tal situación. De pronto te encuentras con estos datos (basado en [CEPLAN](https://www.ceplan.gob.pe/informacion-de-brechas-territoriales/)): 

<iframe width="800" height="400" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pubhtml?"></iframe>

Imaginemos que estos datos son lo 'mejor' que podrás conseguir:

* Variable: Locales Publicos en buen estado:
    - Representación: Porcentaje.
    - Corte: Transversal - sólo 2018.

* Unidad de Observación: Local Público

* Unidad de Análisis: Departamento


Teniendo ello en claro, pasemos a explorar los datos:

1. Carga de datos:

Si los datos están en GoogleDrive, puedes leerlos desde ahí:

```{r}
rm(list = ls()) # limpiar el working environment

linkADrive='https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pub?gid=0&single=true&output=csv'

estadoLocales=read.csv(linkADrive)

head(estadoLocales)
```

La vista preliminar nos muestra varias columnas, pero la de nuestro interés es la última columna de la derecha. A esta altura, antes de explorar los datos de manera estadística, debemos primero verificar cómo R ha intepretado el **tipo** de datos:

```{r}
str(estadoLocales)
```
La columna de interés es de *tipo numérico*, y R también lo tiene intrepretado así. 

2. Tablas, Gráficos, y Estadígrafos

Si los datos están en el tipo adecuado, puede iniciarse la exploración estadística.

2.1 Tablas

Tenemos 25 observaciones a nivel departamental. La **tabla de frecuencias** suele ser una manera básica de ver cómo se distribuyen los datos:

```{r}
library(DescTools)

TF_locales=DescTools::Freq(estadoLocales$LOCALES_PUB_BUENESTADO_2018)
TF_locales

```

La columna "level" muestra intervalos en los cuáles se ha _partido_ o _cortado_ los valores encontrados en la variable. Nótese que desde el segundo intervalo, éste está abierto a la izquierda. 


2.1 Gráficas

La tabla de frecuencias previa puede complementarse con un histograma:

```{r}
hist(estadoLocales$LOCALES_PUB_BUENESTADO_2018)
```
Las funciones **Freq()** y la **hist()** calculan los mismos intervalos. Si usas **ggplot** podrías obtener algo diferente:

```{r}
library(ggplot2)

baseHist= ggplot(data=estadoLocales, aes(x=LOCALES_PUB_BUENESTADO_2018))
histoLocales=baseHist + geom_histogram(bins=8)
histoLocales
```
```{r}
data_histoLocales=ggplot_build(histoLocales)$data[[1]]
data_histoLocales
```
```{r}
data_histoLocales[,c(4,5,2)]
```
Another important plot is the **boxplot**:
```{r}
boxplot(estadoLocales$LOCALES_PUB_BUENESTADO_2018)
```
Y en ggplot:
```{r}
baseBox= ggplot(data=estadoLocales, aes(y=LOCALES_PUB_BUENESTADO_2018))
boxLocales=baseBox + geom_boxplot()
boxLocales
```

2.3 Estadígrafos

La manera más rápida de ver los estadígrafos es con el comando **summary()**:

```{r}
summary(estadoLocales$LOCALES_PUB_BUENESTADO_2018)
```
Este resumen incluye medidas de centralidad, pero no de dispersión, ni de kurtosis ni de asimetría. Para ello tenemos otras librerías:

```{r}
library(psych)
moreStatsLocales=psych::describe(estadoLocales$LOCALES_PUB_BUENESTADO_2018)
moreStatsLocales
```
O...
```{r}
t(moreStatsLocales)
```
Asimismo, no sabemos los valores que salieron **atípicos** en el boxplot, lo cual podemos recuperar así:

```{r}
data_boxLocales=ggplot_build(boxLocales)$data[[1]]
data_boxLocales
```
O de manera más precisa:

```{r}
data_boxLocales$outliers
```
Nota la utilidad de los bigotes de las cajas, cuando hay atípicos:

```{r}
data_boxLocales[c('ymin','ymax')]
```
En este caso, sabemos que los valores que exceden 31.4 serán considerados atípicos.


```{r}
library(ggh4x)

baseHist +
  geom_histogram(aes(y = ..density..),bins=8) +
  stat_theodensity(colour = "red") 

```


```{r}
library(ggpubr)
ggqqplot(estadoLocales$LOCALES_PUB_BUENESTADO_2018)
```
```{r}
shapiro.test(estadoLocales$LOCALES_PUB_BUENESTADO_2018 )
```
```{r}
ks.test(estadoLocales$LOCALES_PUB_BUENESTADO_2018,'pnorm' )
```




<a id='corr'></a>

## II. Análisis Bivariado


Consideremos que nos interesa explorar la posible relación que pueda tener nuestra variable de interés con la cantidad de PEA ocupada.Traigamos esa variable:


```{r}
linkPea="https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pub?gid=1924082402&single=true&output=csv"
peaOcu=read.csv(linkPea)
head(peaOcu)
```
```{r}
gsub(pattern = ",",replacement = "",peaOcu$peaOcupada)
```
```{r}
peaOcu$peaOcupada=gsub(pattern = ",",replacement = "",peaOcu$peaOcupada)
peaOcu$peaOcupada=as.numeric(peaOcu$peaOcupada)
str(peaOcu)
```
```{r}
EstPea=merge(estadoLocales,peaOcu, by = "UBIGEO")
EstPea
```


Como son _dos_ variables de tipo _numérico_ la estrategia a seguir es el análisis de correlación. La gráfica de correlación es esta:

```{r, warning=FALSE, message=FALSE, echo=TRUE}

library(ggplot2)

base=ggplot(data=EstPea, aes(x=peaOcupada, y=LOCALES_PUB_BUENESTADO_2018))
scatter = base + geom_point()
scatter
```
```{r}
library(ggrepel)
scatterText = scatter + geom_text_repel(aes(label=DEPARTAMENTO),size=2)
scatterText
```

Calculemos los indices de correlación:

```{r, warning=FALSE, message=FALSE, echo=TRUE}

f1=formula(~peaOcupada + LOCALES_PUB_BUENESTADO_2018)


```

 
```{r}
# camino parametrico
pearsonf1=cor.test(f1,data=EstPea)[c('estimate','p.value')]
pearsonf1

```

```{r}
# camino no parametrico
spearmanf1=cor.test(f1,data=EstPea,method='spearman',exact=F)[c('estimate','p.value')]
spearmanf1
```

A veces queremos analizar nuestra variable pero organizada por grupos. Por ejemplo, saber si se distribuye igual según la población total. Traigamos los datos de población:

```{r}
linkPobla="https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pub?gid=1758328391&single=true&output=csv"
poblas=read.csv(linkPobla)
head(poblas)
```

```{r}
EstPeaPob=merge(EstPea, poblas,by="UBIGEO")
head(EstPeaPob)
```

```{r}
library(magrittr)
EstPeaPob$masmillonPob=ifelse(EstPeaPob$TOTAL>1000000,'Si','No')%>%as.factor()
table(EstPeaPob$masmillonPob)
```




```{r, warning=FALSE, message=FALSE, echo=TRUE}
base=ggplot(data=EstPeaPob, aes(x=masmillonPob, y=LOCALES_PUB_BUENESTADO_2018))
base + geom_boxplot(notch = T) +  geom_jitter(color="black", size=0.4, alpha=0.9)

```

Los boxplots tienen un _notch_ flanqueando a la _mediana_, para sugerir igualdad de medianas si éstos se intersectan; de ahi que parece no haber diferencia sustantiva entre las regiones.

Una alternativa al boxplot seria las barras de error:
```{r}
ggpubr::ggerrorplot(data=EstPeaPob, x='masmillonPob', y='LOCALES_PUB_BUENESTADO_2018')
```

En este último caso, si las lineas (denotado por las barras de error de la media) se intersectan, sugeriria que los valores medios (en este caso la _media_) podrian ser iguales.

Verificar si hay o no igualdad entre distribuciones depende si las variables se distribuyen o no de manera normal:


```{r}

ggplot(data=EstPeaPob, aes(x=LOCALES_PUB_BUENESTADO_2018)) +
  geom_histogram(aes(y = ..density..),bins=8) +
  ggh4x::stat_theodensity(colour = "red") +
  facet_wrap(~ masmillonPob,ncol = 1)
```


Nota que los histogramas de la data _real_ tienen encima la curva _normal_ que _idealmente_ tendría esa data. La lejanía entre ellos, sugeriría no normalidad.


Se suele usar un qqplot para explorar la presencia/ausencia de normalidad:

```{r, warning=FALSE, message=FALSE, echo=TRUE, eval=TRUE}
# se sugiere normalidad si los puntos no se alejan de la diagonal.

library(ggpubr)
ggqqplot(data=EstPeaPob,x="LOCALES_PUB_BUENESTADO_2018") + facet_grid(~ masmillonPob)
```


Como ello no es fácil de discernir visualmente, tenemos por costumbre calcular algun coeficiente, como el _Shapiro-Wilk_:


```{r, warning=FALSE, message=FALSE, echo=TRUE}


f4=formula(LOCALES_PUB_BUENESTADO_2018~masmillonPob)


tablag= aggregate(f4, EstPeaPob,
          FUN = function(x) {y <- shapiro.test(x); c(y$statistic, y$p.value)})




shapiroTest=as.data.frame(tablag[,2])
names(shapiroTest)=c("W","Prob")
shapiroTest

```

```{r}
# para que se vea mejor:
library(knitr)
library(magrittr)
library(kableExtra)
kable(cbind(tablag[1],shapiroTest))%>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, position = "left")
```

Usemos tanto  la prueba de _Mann-Whitney_ (no paramétrica) como la _prueba t_ para analizar la exploración entre ambas:

```{r, warning=FALSE, message=FALSE, echo=TRUE}
(tf4=t.test(f4,data=EstPeaPob)[c('estimate','p.value')])

```

```{r}
(wilcoxf4=wilcox.test(f4,data=EstPeaPob,exact=F)['p.value'])
```




_____

<a id='rlin'></a>


## Regresión Lineal

La regresión es una técnica donde hay que definir una variable dependiente y una o más independientes. Las independientes pueden tener rol predictor, dependiendo del diseño de investigación, aunque por defecto tiene un rol explicativo. 

La regresión sí quiere informar cuánto una variable (_independiente_) puede explicar la variación de otra (_dependiente_), de ahí que es una técnica para probar hipótesis direccionales o asimétricas (las correlaciones tiene hipótesis simétricas).

La regresión devuelve un modelo, es decir una ecuación, que recoje cómo una o más variables explicarías a otra. Para nuestro caso la variable dependiente es el estado de los locales: 

```{r, warning=FALSE, message=FALSE, echo=TRUE}

modelo1=formula(LOCALES_PUB_BUENESTADO_2018~peaOcupada + masmillonPob)
```

Por ejemplo, para la hipótesis '_el estado de los locales escolares públicos en una región depende de la PEA ocupada regional y si la región tiene más de un millón de pobladores o no_', la regresión arrojaría este resultado:

<br></br>

```{r, eval=FALSE, echo=FALSE}
regre1=lm(modelo1,data = EstPeaPob)
summary(regre1)
```


```{r}
EstPeaPob$peaOcu_pct=EstPeaPob$peaOcupada/EstPeaPob$TOTAL


modelo2=formula(LOCALES_PUB_BUENESTADO_2018~peaOcu_pct + masmillonPob)
regre2=lm(modelo2,data = EstPeaPob)
summary(regre2)
```


O en mejor version con ayuda de _stargazer_:

```{r, warning=FALSE, message=FALSE, echo=TRUE,results='asis'}
library(stargazer)

stargazer(regre2,type = "html",intercept.bottom = FALSE)
```

<br></br>

Aquí ya sabemos algo interesante, **primero** que *peaOcu_pct* tiene efecto, pues es _significativo_ (*p-valor* es menor que 0.05); **segundo**, que ese efecto es _directo_, pues el coeficiente calculado es positivo (signo de columna *Estimate*); y **tercero** que la _magnitud_ de ese efecto es `r round(regre2$coefficients[2],3)`, lo que indica cuanto aumenta, en promedio, la variables dependiente, cuando la variable independiente se incremente en una unidad.

Esto es información suficiente para representar esa relación con una ecuación:

$$  EstadoLocales= `r regre2$coefficients[1]` + `r regre2$coefficients[2]` \cdot PEA + `r regre2$coefficients[3]` \cdot Populoso + \epsilon$$

El Y verdadero es EstadoLocales, pero la regresión produce un $\hat{EstadoLocales}$ estimado, de ahi la presencia del $\epsilon$. Justamente el _R cuadrado ajustado_ (`r summary(regre2)$r.squared`) nos brinda un porcentaje (multiplicalo por 100), que nos da una pista de nuestra cercanía a una situación perfecta (cuando vale **1**).



[al INICIO](#beginning)
