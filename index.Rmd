---
title: "Sesión 1"
output:
  html_document:
    df_print: paged
---

<center><img src="https://github.com/Estadistica-AnalisisPolitico/Images/raw/main/LogoEAP.png" width="500"></center>



Profesor:  <a href="http://www.pucp.edu.pe/profesor/jose-manuel-magallanes/" target="_blank">Dr. José Manuel MAGALLANES REYES, Ph.D.</a> <br>

- Profesor del Departamento de Ciencias Sociales, Sección de Ciencia Política y Gobierno.

- [Oficina 105](https://goo.gl/maps/xuGeG6o9di1i1y5m6) - Edificio CISEPA / ECONOMIA / CCSS

- Telefono: (51) 1 - 6262000 anexo 4302

- Correo Electrónico: [jmagallanes@pucp.edu.pe](mailto:jmagallanes@pucp.edu.pe)
    

<a id='beginning'></a>


____

<center> <header><h2>La Ruta hacia la Regresión</h2>  </header></center>

<center><a href="https://doi.org/10.5281/zenodo.7015029"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.7015029.svg" alt="DOI"></a>
</center>

____

## El _Pipeline_ del Analista Político

Podemos proponer que los _analistas_ existen para brindar __explicaciones__ y __recomendaciones__ sobre algún asunto de interés. Por otro lado, el _decisor_ es responsable de eligir un curso de acción sabiendo que el analista le ha dado __información incompleta__. Ya el _gestor_ se encarga de implementar  las decisiones; esa implementación traerá nueva información y el analista vuelve a su trabajo.

La estadística es una ciencia matemática que guía científicamente el análisis de datos. Así el analista trata de seguir una secuencia en su labor:

1. Apuesta por entender una variable que explicaría un problema de interés (narcotráfico, elecciones, etc). En esta etapa, hace exploración de los datos de esa variable, es decir, organiza los datos en medidas, tablas y gráficos. Se entiende que la variable de interés tiene una variabilidad tal que despierta en el analista la necesidad de preguntar por qué esa variabilidad.

2. Se plantea hipótesis respecto a qué se relaciona con la variabilidad de la variable de interés. Las hipótesis se formula no antes de haber revisado la literatura. En esta etapa hace uso de análisis bivariado o multivariado. Esta etapa se enriquece si está actualizado en las teorías que proponen cierta asociación de la variable de interés con otras variables.


3. Aplica la técnica estadística que corresponda, tal que verifique la hipótesis que se planteó. 

4. Interpreta los resultados obtenidos.

5. Elabora síntesis de lo actuado; propone explicaciones a lo encontrado; y elabora recomendaciones.

Hay muchas opciones para las _técnicas_ señaladas en el punto 3. La elección de las mismas dependerá de la preparación del analista. Esta sesión te guiará para que:

1. Recuerdes cómo y para qué hacer exploración univariada ([ir](#eda))
2. Recuerdes cómo y para qué hacer análisis bivariada ([ir](#corr)).
3. Introducir el concepto (y necesidad) de la regresión multivariada ([ir](#rlin)).


<a id='eda'></a>

## I. Explorando la Variable de interés

Supongamos que estás interesado en la "situación de los locales escolares en el Perú". Ese simple interés te lleva a buscar datos para saber tal situación. De pronto te encuentras con estos datos (basado en [CEPLAN](https://www.ceplan.gob.pe/informacion-de-brechas-territoriales/)): 

<iframe width="800" height="400" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pubhtml?"></iframe>

Imaginemos que estos datos son lo 'mejor' que podrás conseguir:

* Variable: Locales Publicos en buen estado:
    - Representación: Porcentaje.
    - Corte: Transversal - sólo 2018.

* Unidad de Observación: Local Público

* Unidad de Análisis: Departamento


Teniendo ello en claro, pasemos a explorar los datos:

1. Carga de datos:

Si los datos están en GoogleDrive, puedes leerlos desde ahí:

```{r}
rm(list = ls()) # limpiar el working environment

linkADrive='https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pub?gid=0&single=true&output=csv'

estadoLocales=read.csv(linkADrive)

head(estadoLocales)
```

La vista preliminar nos muestra varias columnas, pero la de nuestro interés es la última columna de la derecha. A esta altura, antes de explorar los datos de manera estadística, debemos primero verificar cómo R ha intepretado el **tipo** de datos:

```{r}
str(estadoLocales)
```
La columna de interés es de *tipo numérico*, y R también lo tiene intrepretado así. 

2. Tablas, Gráficos, y Estadígrafos

Si los datos están en el tipo adecuado, puede iniciarse la exploración estadística.

Tenemos 25 observaciones a nivel departamental. La **tabla de frecuencias** suele ser una manera básica de ver cómo se distribuyen los datos:

```{r}
library(DescTools)

TF_locales=DescTools::Freq(estadoLocales$buenEstado)
TF_locales

```

La columna "level" muestra intervalos en los cuáles se ha _partido_ o _cortado_ los valores encontrados en la variable. Nótese que desde el segundo intervalo, éste está abierto a la izquierda. 


La tabla de frecuencias previa puede complementarse con un histograma:

```{r}
hist(estadoLocales$buenEstado)
```

Las funciones **Freq()** y la **hist()** calculan los intervalos con el mismo algoritmo. Si usas **ggplot** obtenemos algo diferente:

```{r}
library(ggplot2)

baseHist= ggplot(data=estadoLocales, aes(x=buenEstado))
histoLocales=baseHist + geom_histogram(bins=8)
histoLocales
```

El algoritmo que usa **ggplot** trae otro resultado. Usando **ggplot_build** podemos ver los datos:

```{r}
data_histoLocales=ggplot_build(histoLocales)$data[[1]]
data_histoLocales
```

Tomemos lo necesario de la tabla anterior para representar cada intervalo y su frecuencia absoluta:

```{r}
data_histoLocales[,c("xmin","xmax","count")]
```

La exploración de variables numéricas debe incluir el uso del **boxplot**:

```{r}
baseBox= ggplot(data=estadoLocales, aes(y=buenEstado))
boxLocales=baseBox + geom_boxplot()
boxLocales
```

El boxplot revela los cuartiles, y lo más importante, la presencia de atípicos (outliers). 

La manera más rápida de ver los estadígrafos es usando el comando **summary()**:

```{r}
summary(estadoLocales$buenEstado)
```

Este resumen incluye medidas de centralidad y posición, pero no de dispersión, ni de kurtosis, ni de asimetría. Para ello tenemos, entre otras opciones, la función _Desc_ del paquete _DescTools_:

```{r}
library(DescTools)
moreStatsLocales=DescTools::Desc(estadoLocales$buenEstado)
moreStatsLocales
```


A esta altura, no sabemos los valores que salieron **atípicos** en el boxplot, lo cual podemos recuperar con *ggplot_build*:

```{r}
data_boxLocales=ggplot_build(boxLocales)$data[[1]]
data_boxLocales
```
Los outliers están en una lista, por lo que debemos escribir:

```{r}
data_boxLocales$outliers
```
Nota la utilidad de los "bigotes" del boxplot, cuando hay atípicos:

```{r}
data_boxLocales[c('ymin','ymax')]
```
En este caso, sabemos que los valores que exceden 31.4 serán considerados atípicos.

A esta altura, tienes en claro que hay asimetría, y quisieras ver si el histograma se aleja de la normalidad:

```{r}
library(ggh4x)

baseHist +
  geom_histogram(aes(y =after_stat(density)),bins=8) +
  stat_theodensity(colour = "red") 

```

El análisis de la normalidad también puede incluir el **qqplot**:

```{r}
library(ggpubr)
ggqqplot(estadoLocales$buenEstado)
```

Lo cuál se acompaña con los tests de normalidad (Ho: data es normal):

```{r}
shapiro.test(estadoLocales$buenEstado )
```
```{r}
ks.test(estadoLocales$buenEstado,'pnorm')
```

Hasta aquí, podrias sustentar que los valores de la variable buen estado de colegios públicos a nivel regional se distribuyen asimétricamente, con una dispersión baja, y con presencia de valores atípicos altos.


<a id='corr'></a>

## II. Análisis Bivariado


Consideremos que nos interesa explorar la posible relación entre nuestra variable de interés y la PEA ocupada. Traigamos esa variable:


```{r}
linkPea="https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pub?gid=1924082402&single=true&output=csv"
peaOcu=read.csv(linkPea)
head(peaOcu)
```

Nótese que los valores numéricos han sido interpretados como texto. Las comas lo han causado, pero podemos eliminarlas así:

```{r}
gsub(pattern = ",",replacement = "",peaOcu$peaOcupada)
```

Usemos ese código para volver numérica esa columna:

```{r}
peaOcu$peaOcupada=gsub(pattern = ",",replacement = "",peaOcu$peaOcupada)
peaOcu$peaOcupada=as.numeric(peaOcu$peaOcupada)

# veamos
str(peaOcu)
```
Los datos de la PEA están en una tabla diferente, por lo que debemos juntar (merge) ambas tablas, usando como _key_ alguna columna común en ambas:

```{r}
EstPea=merge(estadoLocales,peaOcu, by = "UBIGEO")
EstPea
```


Como son _dos_ variables de tipo _numérico_ la estrategia a seguir es el análisis de correlación. Veamos este **scatterplot**:

```{r, warning=FALSE, message=FALSE, echo=TRUE}

library(ggplot2)

base=ggplot(data=EstPea, aes(x=peaOcupada, y=buenEstado))
scatter = base + geom_point()
scatter
```

Como tenemos pocos casos, podemos poner texto:

```{r}
library(ggrepel)
scatterText = scatter + geom_text_repel(aes(label=DEPARTAMENTO),size=2)
scatterText
```

Calculemos ahora los indices de correlación:

```{r, warning=FALSE, message=FALSE, echo=TRUE}
f1=formula(~peaOcupada + buenEstado)
```

Camino parametrico:

```{r}

pearsonf1=cor.test(f1,data=EstPea)[c('estimate','p.value')]
pearsonf1

```

Camino no parametrico

```{r}
spearmanf1=cor.test(f1,data=EstPea,method='spearman',exact=F)[c('estimate','p.value')]
spearmanf1
```

Hasta aquí, dudamos que esas variables estén correlacionadas.

Otro caso importante es cuando analizamos nuestra variable versus una variable categórica. Por ejemplo, saber si se distribuye igual según una región sea "populosa" o no lo sea. Traigamos los datos de población:

```{r}
linkPobla="https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pub?gid=1758328391&single=true&output=csv"
poblas=read.csv(linkPobla)
head(poblas)
```

De nuevo debemos hacer _merge_:

```{r}
EstPeaPob=merge(EstPea, poblas,by="UBIGEO")
head(EstPeaPob)
```

Creemos aquí una variable categórica (_factor_) que represente "populoso" como tener más de un millón de habitantes:

```{r}
library(magrittr)
EstPeaPob$populoso=ifelse(EstPeaPob$TOTAL>1000000,'Si','No')%>%as.factor()

# veamos conteo de cada categoría
table(EstPeaPob$populoso)
```

Pidamos un boxplot, pero por grupos:

```{r, warning=FALSE, message=FALSE, echo=TRUE}
base=ggplot(data=EstPeaPob, aes(x=populoso, y=buenEstado))
base + geom_boxplot(notch = T) +  geom_jitter(color="black", size=0.4, alpha=0.9)

```

Los boxplots tienen un _notch_ flanqueando a la _mediana_, para sugerir igualdad de medianas si éstos se intersectan; de ahi que parece no haber diferencia sustantiva entre las categorías.

Una alternativa al boxplot seria las barras de error:
```{r}
ggpubr::ggerrorplot(data=EstPeaPob, x='populoso', y='buenEstado')
```

En este último caso, si las líneas (denotado por las barras de error de la media) se interceptan, sugeriría que los valores medios (en este caso la _media_) podrían ser iguales.

Verificar si hay o no igualdad entre distribuciones depende si las variables se distribuyen o no de manera normal por grupo:


```{r}

ggplot(data=EstPeaPob, aes(x=buenEstado)) +
  geom_histogram(aes(y = ..density..),bins=8) +
  ggh4x::stat_theodensity(colour = "red") +
  facet_wrap(~ populoso,ncol = 1)
```


Nota que los histogramas de la data _real_ tienen encima la curva _normal_ que _idealmente_ tendría esa data. La lejanía entre ellos, sugeriría no normalidad.


Volvamos a usar un qqplot para explorar la presencia/ausencia de normalidad por grupo:

```{r, warning=FALSE, message=FALSE, echo=TRUE, eval=TRUE}
# se sugiere normalidad si los puntos no se alejan de la diagonal.

library(ggpubr)
ggqqplot(data=EstPeaPob,x="buenEstado") + facet_grid(~ populoso)
```


Como ello no es fácil de discernir visualmente, complementemos el análisis con los tests de normalidad. Usemos _Shapiro-Wilk_ en cada grupo:


```{r, warning=FALSE, message=FALSE, echo=TRUE}


f2=formula(buenEstado~populoso)


tablag= aggregate(f2, EstPeaPob,
          FUN = function(x) {y <- shapiro.test(x); c(y$statistic, y$p.value)})




shapiroTest=as.data.frame(tablag[,2])
names(shapiroTest)=c("W","Prob")
shapiroTest

```
Para que se vea mejor en _html_:
```{r}
library(knitr)
library(magrittr)
library(kableExtra)
kable(cbind(tablag[1],shapiroTest))%>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, position = "left")
```

Habría normalidad en un grupo y no en otro. Usemos entonces tanto  la prueba de _Mann-Whitney_ (no paramétrica) como la _prueba t_ para analizar las diferencias:

```{r, warning=FALSE, message=FALSE, echo=TRUE}
(student_T=t.test(f2,data=EstPeaPob)[c('estimate','p.value')])

```

```{r}
(Mann_Whitnery=wilcox.test(f2,data=EstPeaPob,exact=F)['p.value'])
```
Con toda esta información, iriamos concluyendo también que ser populoso o no no tendría efecto en nuestra variable.



_____

<a id='rlin'></a>


## Regresión Lineal

La regresión es una técnica donde hay que definir una variable dependiente y una o más independientes. Las independientes pueden tener rol predictor, dependiendo del diseño de investigación, aunque por defecto tiene un rol explicativo. 

La regresión sí quiere informar cuánto una variable (_independiente_) puede explicar la variación de otra (_dependiente_), de ahí que es una técnica para probar hipótesis direccionales o asimétricas (las correlaciones tiene hipótesis simétricas).

La regresión devuelve un modelo, es decir una ecuación, que recoge cómo una o más variables explicaríab a otra. Para nuestro caso la variable dependiente es el estado de los locales: 

```{r, warning=FALSE, message=FALSE, echo=TRUE}

modelo1=formula(buenEstado~peaOcupada + populoso)
```

Por ejemplo, para la hipótesis '_el estado de los locales escolares públicos en una región depende de la PEA ocupada regional y si la región tiene más de un millón de pobladores o no_', la regresión arrojaría este resultado:

<br></br>

```{r, eval=FALSE, echo=FALSE}
regre1=lm(modelo1,data = EstPeaPob)
summary(regre1)
```
Hasta aquí, vemos que lo que nos informaba el análisis bivariado se mantiene en la regresión: ningun predictor tiene efecto, pues la probabilidad de que el efecto sea cero, en cada caso, es muy alta (mayor a 0.1).

Sin embargo, es aquí donde reflexionamos si los datos crudos que tenemos podrían necesitar alguna **transformación**:

```{r}
# la pea como porcentaje
EstPeaPob$peaOcu_pct=EstPeaPob$peaOcupada/EstPeaPob$TOTAL


modelo2=formula(buenEstado~peaOcu_pct + populoso)
regre2=lm(modelo2,data = EstPeaPob)
summary(regre2)
```


O en mejor version con ayuda de _stargazer_:

```{r, warning=FALSE, message=FALSE, echo=TRUE,results='asis'}
library(stargazer)

stargazer(regre2,type = "html",intercept.bottom = FALSE)
```

<br></br>

Aquí aparace algo muy interesante, **primero** que *peaOcu_pct* tiene efecto, pues es _significativo_ (*p-valor* es menor que 0.05); **segundo**, que ese efecto es _directo_, pues el coeficiente calculado es positivo (signo de columna *Estimate*); y **tercero** que la _magnitud_ de ese efecto es `r round(regre2$coefficients[2],3)`, lo que indica cuánto aumenta, en promedio, la variables dependiente, cuando la variable independiente se incremente en una unidad.

Esto es información suficiente para representar esa relación con una ecuación:

$$  BuenEstado\_pct= `r regre2$coefficients[1]` + `r regre2$coefficients[2]` \cdot PEA\_pct + `r regre2$coefficients[3]` \cdot Populoso + \epsilon$$

El Y verdadero es *BuenEstado_pct*, pero la regresión produce un $\hat{BuenEstado_pct}$ estimado, de ahi la presencia del $\epsilon$. Justamente el _R cuadrado ajustado_ (`r summary(regre2)$r.squared`) nos brinda un porcentaje (multiplicalo por 100), que nos da una pista de nuestra cercanía a una situación perfecta (cuando vale **1**).



[al INICIO](#beginning)
